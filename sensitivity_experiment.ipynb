{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from libs.models import encoder3,encoder4\n",
    "from libs.models import decoder3,decoder4\n",
    "import numpy as np\n",
    "from libs.Matrix import MulLayer\n",
    "from libs.Criterion import LossCriterion\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "class LossCriterion(nn.Module):\n",
    "    def __init__(self, style_layers, content_layers, style_weight, content_weight):\n",
    "        super(LossCriterion, self).__init__()\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.style_weight = style_weight\n",
    "        self.content_weight = content_weight\n",
    "        self.styleLosses = [styleLoss()] * len(style_layers)\n",
    "        self.contentLosses = [nn.MSELoss()] * len(content_layers)\n",
    "\n",
    "    def forward(self, tF, sF, cF):\n",
    "        # Content loss\n",
    "        totalContentLoss = 0\n",
    "        for i, layer in enumerate(self.content_layers):\n",
    "            cf_i = cF[layer].detach()\n",
    "            tf_i = tF[layer]\n",
    "            loss_i = self.contentLosses[i]\n",
    "            totalContentLoss += loss_i(tf_i, cf_i)\n",
    "        totalContentLoss = totalContentLoss * self.content_weight\n",
    "\n",
    "        # Style loss\n",
    "        \n",
    "        totalStyleLoss = 0\n",
    "        for i, layer in enumerate(self.style_layers):\n",
    "            sf_i = sF[layer].detach()\n",
    "            tf_i = tF[layer]\n",
    "            loss_i = self.styleLosses[i]\n",
    "            totalStyleLoss += loss_i(tf_i, sf_i)\n",
    "        totalStyleLoss = totalStyleLoss * self.style_weight\n",
    "\n",
    "        loss = totalStyleLoss + totalContentLoss\n",
    "        return loss, totalStyleLoss, totalContentLoss\n",
    "\n",
    "\n",
    "class styleLoss(nn.Module):\n",
    "    def forward(self,input,target):\n",
    "        ib,ic,ih,iw = input.size()\n",
    "        iF = input.view(ib,ic,-1)\n",
    "        iMean = torch.mean(iF,dim=2)\n",
    "        iCov = GramMatrix()(input)\n",
    "\n",
    "        tb,tc,th,tw = target.size()\n",
    "        tF = target.view(tb,tc,-1)\n",
    "        tMean = torch.mean(tF,dim=2)\n",
    "        tCov = GramMatrix()(target)\n",
    "\n",
    "        loss = nn.MSELoss(size_average=False)(iMean,tMean) + nn.MSELoss(size_average=False)(iCov,tCov)\n",
    "        return loss/tb\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self,input):\n",
    "        b, c, h, w = input.size()\n",
    "        f = input.view(b,c,h*w) # bxcx(hxw)\n",
    "        # torch.bmm(batch1, batch2, out=None)   #\n",
    "        # batch1: bxmxp, batch2: bxpxn -> bxmxn #\n",
    "        G = torch.bmm(f,f.transpose(1,2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n",
    "        return G.div_(c*h*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from libs.Loader import Dataset\n",
    "from libs.models import encoder4, decoder4\n",
    "from libs.Criterion import LossCriterion\n",
    "\n",
    "\n",
    "\n",
    "class Loss_sensitivity:\n",
    "    def __init__(self, vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device):\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.dec = dec.to(device)\n",
    "        self.matrix = matrix.to(device)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.criterion = LossCriterion(style_layers, content_layers, style_weight, content_weight)\n",
    "        self.device = device\n",
    "\n",
    "    def add_noise(self, matrix, sigma):\n",
    "        noise = torch.randn_like(matrix) * sigma\n",
    "        return matrix + noise\n",
    "\n",
    "    def forward(self, contentV, styleV):\n",
    "        with torch.no_grad():\n",
    "            sF = self.vgg(styleV)\n",
    "            cF = self.vgg(contentV)\n",
    "        return sF, cF\n",
    "\n",
    "    def run_experiment(self, contentV, styleV, sigmas):\n",
    "        sigma_values = []\n",
    "        loss_values = []\n",
    "\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "\n",
    "        for sigma in sigmas:\n",
    "            \n",
    "            transformed_features, transmatrix = self.matrix(cF[self.style_layers[0]], sF[self.style_layers[0]])\n",
    "            \n",
    "            # Add noise to the transformation matrix\n",
    "            noisy_transmatrix = self.add_noise(transmatrix, sigma)\n",
    "            \n",
    "            # Apply the noisy transformation matrix\n",
    "            b, c, h, w = transformed_features.size()\n",
    "            compressed_features = self.matrix.compress(transformed_features)\n",
    "            noisy_transfeature = torch.bmm(noisy_transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            noisy_transfeature = noisy_transfeature.view(b, self.matrix.matrixSize, h, w)\n",
    "            noisy_transfeature = self.matrix.unzip(noisy_transfeature)\n",
    "            \n",
    "            noisy_transfer = self.dec(noisy_transfeature).clamp(0, 1)\n",
    "\n",
    "            tF = self.vgg(noisy_transfer)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "\n",
    "            sigma_values.append(sigma)\n",
    "            loss_values.append(total_loss.item())\n",
    "            print(f\"Sigma: {sigma}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "        return sigma_values, loss_values\n",
    "    \n",
    "    def run_scaling_vector_experiment(self, contentV, styleV, scale_factors, vector_magnitudes):\n",
    "        scale_values = []\n",
    "        vector_values = []\n",
    "        loss_values = []\n",
    "\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "\n",
    "        # Get the original transformation matrix\n",
    "        _, transmatrix = self.matrix(cF[self.style_layers[0]], sF[self.style_layers[0]])\n",
    "\n",
    "        for scale in scale_factors:\n",
    "            for vector_mag in vector_magnitudes:\n",
    "                # Scale the transformation matrix\n",
    "                scaled_matrix = transmatrix * scale\n",
    "\n",
    "                # Create a simple vector and add it to each row of the matrix\n",
    "                vector = torch.ones_like(scaled_matrix[0, 0]) * vector_mag\n",
    "                modified_matrix = scaled_matrix + vector.unsqueeze(1)\n",
    "\n",
    "                # Apply the modified transformation\n",
    "                b, c, h, w = cF[self.style_layers[0]].size()\n",
    "                compressed_features = self.matrix.compress(cF[self.style_layers[0]])\n",
    "                modified_features = torch.bmm(modified_matrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "                modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "                modified_features = self.matrix.unzip(modified_features)\n",
    "\n",
    "                modified_transfer = self.dec(modified_features).clamp(0, 1)\n",
    "\n",
    "                tF = self.vgg(modified_transfer)\n",
    "                total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "\n",
    "                scale_values.append(scale)\n",
    "                vector_values.append(vector_mag)\n",
    "                loss_values.append(total_loss.item())\n",
    "                print(f\"Scale: {scale}, Vector Magnitude: {vector_mag}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "        return scale_values, vector_values, loss_values\n",
    "    \n",
    "    def run_dimension_dropping_experiment(self, contentV, styleV):\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "        \n",
    "        # Get the original features\n",
    "        content_features = cF[self.style_layers[0]]\n",
    "        style_features = sF[self.style_layers[0]]\n",
    "        \n",
    "        b, c, h, w = content_features.size()\n",
    "        \n",
    "        content_loss_impacts = []\n",
    "        style_loss_impacts = []\n",
    "        \n",
    "        # Calculate baseline loss\n",
    "        _, transmatrix = self.matrix(content_features, style_features)\n",
    "        compressed_features = self.matrix.compress(content_features)\n",
    "        modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "        modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "        modified_features = self.matrix.unzip(modified_features)\n",
    "        stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "        tF = self.vgg(stylized_image)\n",
    "        baseline_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "        \n",
    "        # Analyze content dimensions\n",
    "        for dim in range(c):\n",
    "            dropped_content = content_features.clone()\n",
    "            dropped_content[:, dim, :, :] = 0\n",
    "            \n",
    "            # Apply the transformation\n",
    "            _, transmatrix = self.matrix(dropped_content, style_features)\n",
    "            compressed_features = self.matrix.compress(dropped_content)\n",
    "            modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "            modified_features = self.matrix.unzip(modified_features)\n",
    "            \n",
    "            # Generate the stylized image and calculate loss\n",
    "            stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "            tF = self.vgg(stylized_image)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            \n",
    "            content_loss_impacts.append(total_loss.item() - baseline_loss.item())\n",
    "        \n",
    "        # Analyze style dimensions\n",
    "        for dim in range(c):\n",
    "            dropped_style = style_features.clone()\n",
    "            dropped_style[:, dim, :, :] = 0\n",
    "            \n",
    "            # Apply the transformation\n",
    "            _, transmatrix = self.matrix(content_features, dropped_style)\n",
    "            compressed_features = self.matrix.compress(content_features)\n",
    "            modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "            modified_features = self.matrix.unzip(modified_features)\n",
    "            \n",
    "            # Generate the stylized image and calculate loss\n",
    "            stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "            tF = self.vgg(stylized_image)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            \n",
    "            style_loss_impacts.append(total_loss.item() - baseline_loss.item())\n",
    "        \n",
    "        return content_loss_impacts, style_loss_impacts\n",
    "    \n",
    "    def run_pca_experiment(self, contentV, styleV, n_components=50):\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "        \n",
    "        # Get the original features\n",
    "        content_features = cF[self.style_layers[0]]\n",
    "        style_features = sF[self.style_layers[0]]\n",
    "        \n",
    "        b, c, h, w = content_features.size()\n",
    "        \n",
    "        # Reshape features for PCA\n",
    "        content_features_flat = content_features.view(b, c, -1).permute(0, 2, 1).reshape(-1, c)\n",
    "        style_features_flat = style_features.view(b, c, -1).permute(0, 2, 1).reshape(-1, c)\n",
    "        \n",
    "        # Apply PCA\n",
    "        content_pca = PCA(n_components=n_components)\n",
    "        style_pca = PCA(n_components=n_components)\n",
    "        \n",
    "        content_pca.fit(content_features_flat.cpu().numpy())\n",
    "        style_pca.fit(style_features_flat.cpu().numpy())\n",
    "        \n",
    "        # Calculate baseline loss\n",
    "        _, transmatrix = self.matrix(content_features, style_features)\n",
    "        compressed_features = self.matrix.compress(content_features)\n",
    "        modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "        modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "        modified_features = self.matrix.unzip(modified_features)\n",
    "        stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "        tF = self.vgg(stylized_image)\n",
    "        baseline_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "        \n",
    "        content_pca_loss_impacts = []\n",
    "        style_pca_loss_impacts = []\n",
    "        \n",
    "        # Analyze PCA components for content\n",
    "        for i in range(n_components):\n",
    "            pca_component = torch.from_numpy(content_pca.components_[i]).float().to(self.device)\n",
    "            content_flat = content_features.view(b, c, -1)\n",
    "            projection = torch.matmul(content_flat.permute(0, 2, 1), pca_component.unsqueeze(1)).permute(0, 2, 1)\n",
    "            dropped_content_flat = content_flat - projection * pca_component.unsqueeze(0).unsqueeze(-1)\n",
    "            dropped_content = dropped_content_flat.view(b, c, h, w)\n",
    "            \n",
    "            # Apply the transformation\n",
    "            _, transmatrix = self.matrix(dropped_content, style_features)\n",
    "            compressed_features = self.matrix.compress(dropped_content)\n",
    "            modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "            modified_features = self.matrix.unzip(modified_features)\n",
    "            \n",
    "            # Generate the stylized image and calculate loss\n",
    "            stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "            tF = self.vgg(stylized_image)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            \n",
    "            content_pca_loss_impacts.append(total_loss.item() - baseline_loss.item())\n",
    "        \n",
    "        # Analyze PCA components for style (similar changes as content)\n",
    "        for i in range(n_components):\n",
    "            pca_component = torch.from_numpy(style_pca.components_[i]).float().to(self.device)\n",
    "            style_flat = style_features.view(b, c, -1)\n",
    "            projection = torch.matmul(style_flat.permute(0, 2, 1), pca_component.unsqueeze(1)).permute(0, 2, 1)\n",
    "            dropped_style_flat = style_flat - projection * pca_component.unsqueeze(0).unsqueeze(-1)\n",
    "            dropped_style = dropped_style_flat.view(b, c, h, w)\n",
    "            \n",
    "            # Apply the transformation\n",
    "            _, transmatrix = self.matrix(content_features, dropped_style)\n",
    "            compressed_features = self.matrix.compress(content_features)\n",
    "            modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "            modified_features = self.matrix.unzip(modified_features)\n",
    "            \n",
    "            # Generate the stylized image and calculate loss\n",
    "            stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "            tF = self.vgg(stylized_image)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            \n",
    "            style_pca_loss_impacts.append(total_loss.item() - baseline_loss.item())\n",
    "        \n",
    "        return content_pca_loss_impacts, style_pca_loss_impacts, content_pca, style_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m             plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_LAUNCH_BLOCKING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m vgg \u001b[38;5;241m=\u001b[39m \u001b[43mencoder4\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m dec \u001b[38;5;241m=\u001b[39m decoder4()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     96\u001b[0m matrix \u001b[38;5;241m=\u001b[39m MulLayer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr41\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from libs.Loader import Dataset\n",
    "from libs.models import encoder4, decoder4\n",
    "from libs.Criterion import LossCriterion\n",
    "from libs.Matrix import MulLayer\n",
    "import os\n",
    "\n",
    "class Loss_sensitivity:\n",
    "    def __init__(self, vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device):\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.dec = dec.to(device)\n",
    "        self.matrix = matrix.to(device)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.criterion = LossCriterion(style_layers, content_layers, style_weight, content_weight).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def add_noise(self, matrix, sigma):\n",
    "        if matrix.shape != (1, 32, 32, 32):\n",
    "            matrix = matrix.view(1, 32, 32, 32)\n",
    "        return matrix + torch.randn_like(matrix) * sigma\n",
    "\n",
    "    def forward(self, contentV, styleV):\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                sF = self.vgg(styleV)\n",
    "                cF = self.vgg(contentV)\n",
    "            except RuntimeError as e:\n",
    "                print(\"RuntimeError during forward pass (likely GPU issue):\", e)\n",
    "                raise\n",
    "        return sF, cF\n",
    "\n",
    "    def compute_loss(self, contentV, styleV, noisy_matrix):\n",
    "        try:\n",
    "            sF, cF = self.forward(contentV, styleV)\n",
    "            \n",
    "            transformed_features, _ = self.matrix(cF[self.style_layers[0]], sF[self.style_layers[0]])\n",
    "            b, c, h, w = transformed_features.size()\n",
    "            compressed_features = self.matrix.compress(transformed_features)\n",
    "\n",
    "            # Reshape noisy_matrix to match the expected dimensions\n",
    "            noisy_matrix = noisy_matrix.view(b, 32, 32, 32)\n",
    "\n",
    "            noisy_transfeature = torch.einsum('bijk,bjkl->bil', noisy_matrix.to(self.device), compressed_features.view(b, 32, 32, -1))\n",
    "            noisy_transfeature = noisy_transfeature.view(b, c, h, w)\n",
    "            noisy_transfeature = self.matrix.unzip(noisy_transfeature)\n",
    "\n",
    "            noisy_transfer = self.dec(noisy_transfeature).clamp(0, 1)\n",
    "            tF = self.vgg(noisy_transfer)\n",
    "\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            return total_loss.item()\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            print(\"RuntimeError during compute_loss:\", e)\n",
    "            torch.cuda.empty_cache()\n",
    "            return float('inf')\n",
    "\n",
    "    def run_experiment(self, contentV, styleV, sigmas, matrix):\n",
    "        sigma_values = []\n",
    "        loss_values = []\n",
    "\n",
    "        for sigma in sigmas:\n",
    "            noisy_matrix = self.add_noise(matrix, sigma)\n",
    "            loss = self.compute_loss(contentV, styleV, noisy_matrix)\n",
    "            if loss == float('inf'):\n",
    "                print(f\"Skipping sigma {sigma} due to dimension mismatch or GPU issue.\")\n",
    "                continue\n",
    "            sigma_values.append(sigma)\n",
    "            loss_values.append(loss)\n",
    "            print(f\"Sigma: {sigma}, Total Loss: {loss}\")\n",
    "\n",
    "        return sigma_values, loss_values\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    import os\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    # Load models\n",
    "    vgg = encoder4().to(device)\n",
    "    dec = decoder4().to(device)\n",
    "    matrix = MulLayer('r41').to(device)\n",
    "    vgg_dir = 'models/vgg_r41.pth'\n",
    "    decoder_dir = \"models/dec_r41.pth\"\n",
    "    vgg.load_state_dict(torch.load(vgg_dir, map_location=device))\n",
    "    dec.load_state_dict(torch.load(decoder_dir, map_location=device))\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "\n",
    "    class Options:\n",
    "        def __init__(self):\n",
    "            self.contentPath = \"data/content/\"\n",
    "            self.stylePath = \"data/style/\"\n",
    "            self.loadSize = 256\n",
    "            self.fineSize = 256\n",
    "            self.matrixPath = \"Matrices/\"  # Path where matrices are stored\n",
    "\n",
    "    opt = Options()\n",
    "    loss_sensitivity = Loss_sensitivity(vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device)\n",
    "\n",
    "    # Generate larger set of sigmas\n",
    "    sigmas = np.linspace(0, 0.2, 100)\n",
    "\n",
    "    # Load all saved matrices grouped by style\n",
    "    style_dirs = [d for d in os.listdir(opt.matrixPath) if os.path.isdir(os.path.join(opt.matrixPath, d))]\n",
    "\n",
    "    for style_dir in style_dirs:\n",
    "        style_path = os.path.join(opt.matrixPath, style_dir)\n",
    "        matrix_files = [f for f in os.listdir(style_path) if f.endswith('.pth')]\n",
    "        all_loss_values = []\n",
    "\n",
    "        for matrix_file in matrix_files:\n",
    "            matrix_path = os.path.join(style_path, matrix_file)\n",
    "            saved_matrix = torch.load(matrix_path, map_location=device)\n",
    "\n",
    "            # Load a single content and style image for the experiment\n",
    "            content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "            style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "            contentV, _ = content_dataset[0]\n",
    "            styleV, _ = style_dataset[0]\n",
    "            contentV = contentV.unsqueeze(0).to(device)\n",
    "            styleV = styleV.unsqueeze(0).to(device)\n",
    "\n",
    "            _, loss_values = loss_sensitivity.run_experiment(contentV, styleV, sigmas, saved_matrix)\n",
    "            all_loss_values.append(loss_values)\n",
    "\n",
    "        # Calculate average loss values for the current style\n",
    "        if all_loss_values:  # Check if there are valid loss values\n",
    "            avg_loss_values = np.mean(all_loss_values, axis=0)\n",
    "\n",
    "            # Plot the results for the current style\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(sigmas[:len(avg_loss_values)], avg_loss_values, '-o')\n",
    "            plt.xlabel('Sigma (Noise Level)')\n",
    "            plt.ylabel('Average Total Loss')\n",
    "            plt.title(f'Average Noise Sensitivity for Style: {style_dir}')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'average_noise_sensitivity_{style_dir}.png')\n",
    "            plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load models\n",
    "    vgg = encoder4()\n",
    "    dec = decoder4()\n",
    "    matrix = MulLayer('r41')\n",
    "    \n",
    "    vgg_dir = 'models/vgg_r41.pth'\n",
    "    decoder_dir = \"models/dec_r41.pth\"\n",
    "    \n",
    "    vgg.load_state_dict(torch.load(vgg_dir))\n",
    "    dec.load_state_dict(torch.load(decoder_dir))\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "    \n",
    "    class Options:\n",
    "        def __init__(self):\n",
    "            self.contentPath = \"data/content/\"\n",
    "            self.stylePath = \"data/style/\"\n",
    "            self.loadSize = 256\n",
    "            self.fineSize = 256\n",
    "\n",
    "    opt = Options()\n",
    "    \n",
    "    loss_sensitivity = Loss_sensitivity(vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device)\n",
    "    \n",
    "    # Load a single content and style image for the experiment\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    \n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    \n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run scaling and vector addition experiment\n",
    "    scale_factors = np.linspace(0.5, 1.5, 5)\n",
    "    vector_magnitudes = np.linspace(0, 0.1, 5)\n",
    "    scale_values, vector_values, loss_values = loss_sensitivity.run_scaling_vector_experiment(contentV, styleV, scale_factors, vector_magnitudes)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(scale_values, vector_values, loss_values, c=loss_values, cmap='viridis')\n",
    "    ax.set_xlabel('Scale Factor')\n",
    "    ax.set_ylabel('Vector Magnitude')\n",
    "    ax.set_zlabel('Total Loss')\n",
    "    ax.set_title('Loss Sensitivity to Scaling and Vector Addition')\n",
    "    fig.colorbar(scatter, label='Total Loss')\n",
    "    plt.savefig('scaling_vector_sensitivity_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load models\n",
    "    vgg = encoder4()\n",
    "    dec = decoder4()\n",
    "    matrix = MulLayer('r41')\n",
    "    \n",
    "    vgg_dir = 'models/vgg_r41.pth'\n",
    "    decoder_dir = \"models/dec_r41.pth\"\n",
    "    \n",
    "    vgg.load_state_dict(torch.load(vgg_dir))\n",
    "    dec.load_state_dict(torch.load(decoder_dir))\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "    \n",
    "    class Options:\n",
    "        def __init__(self):\n",
    "            self.contentPath = \"data/content/\"\n",
    "            self.stylePath = \"data/style/\"\n",
    "            self.loadSize = 256\n",
    "            self.fineSize = 256\n",
    "\n",
    "    opt = Options()\n",
    "    \n",
    "    loss_sensitivity = Loss_sensitivity(vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device)\n",
    "    \n",
    "    # Load a single content and style image for the experiment\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    \n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    \n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run dimension dropping experiment\n",
    "    content_loss_impacts, style_loss_impacts = loss_sensitivity.run_dimension_dropping_experiment(contentV, styleV)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16))\n",
    "    \n",
    "    ax1.plot(range(len(content_loss_impacts)), content_loss_impacts)\n",
    "    ax1.set_xlabel('Content Feature Dimension')\n",
    "    ax1.set_ylabel('Change in Total Loss')\n",
    "    ax1.set_title('Impact of Dropping Individual Content Feature Dimensions on Total Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(range(len(style_loss_impacts)), style_loss_impacts)\n",
    "    ax2.set_xlabel('Style Feature Dimension')\n",
    "    ax2.set_ylabel('Change in Total Loss')\n",
    "    ax2.set_title('Impact of Dropping Individual Style Feature Dimensions on Total Loss')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dimension_dropping_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # ... (previous setup code remains unchanged) ...\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load models\n",
    "    vgg = encoder4()\n",
    "    dec = decoder4()\n",
    "    matrix = MulLayer('r41')\n",
    "    \n",
    "    vgg_dir = 'models/vgg_r41.pth'\n",
    "    decoder_dir = \"models/dec_r41.pth\"\n",
    "    \n",
    "    vgg.load_state_dict(torch.load(vgg_dir))\n",
    "    dec.load_state_dict(torch.load(decoder_dir))\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "    \n",
    "    class Options:\n",
    "        def __init__(self):\n",
    "            self.contentPath = \"data/content/\"\n",
    "            self.stylePath = \"data/style/\"\n",
    "            self.loadSize = 256\n",
    "            self.fineSize = 256\n",
    "\n",
    "    opt = Options()\n",
    "    loss_sensitivity = Loss_sensitivity(vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device)\n",
    "    \n",
    "    # Load a single content and style image for the experiment\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    \n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    \n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run dimension dropping experiment\n",
    "    content_loss_impacts, style_loss_impacts = loss_sensitivity.run_dimension_dropping_experiment(contentV, styleV)\n",
    "    \n",
    "    # Run PCA experiment\n",
    "    n_components = 50\n",
    "    content_pca_loss_impacts, style_pca_loss_impacts, content_pca, style_pca = loss_sensitivity.run_pca_experiment(contentV, styleV, n_components)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    \n",
    "    ax1.plot(range(len(content_loss_impacts)), content_loss_impacts)\n",
    "    ax1.set_xlabel('Content Feature Dimension')\n",
    "    ax1.set_ylabel('Change in Total Loss')\n",
    "    ax1.set_title('Impact of Dropping Individual Content Feature Dimensions')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(range(len(style_loss_impacts)), style_loss_impacts)\n",
    "    ax2.set_xlabel('Style Feature Dimension')\n",
    "    ax2.set_ylabel('Change in Total Loss')\n",
    "    ax2.set_title('Impact of Dropping Individual Style Feature Dimensions')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    ax3.plot(range(n_components), content_pca_loss_impacts)\n",
    "    ax3.set_xlabel('Content PCA Component')\n",
    "    ax3.set_ylabel('Change in Total Loss')\n",
    "    ax3.set_title('Impact of Dropping Content PCA Components')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    ax4.plot(range(n_components), style_pca_loss_impacts)\n",
    "    ax4.set_xlabel('Style PCA Component')\n",
    "    ax4.set_ylabel('Change in Total Loss')\n",
    "    ax4.set_title('Impact of Dropping Style PCA Components')\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dimension_and_pca_analysis_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    ax1.plot(range(1, n_components + 1), content_pca.explained_variance_ratio_)\n",
    "    ax1.set_xlabel('Number of Components')\n",
    "    ax1.set_ylabel('Explained Variance Ratio')\n",
    "    ax1.set_title('Explained Variance Ratio of Content PCA Components')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(range(1, n_components + 1), style_pca.explained_variance_ratio_)\n",
    "    ax2.set_xlabel('Number of Components')\n",
    "    ax2.set_ylabel('Explained Variance Ratio')\n",
    "    ax2.set_title('Explained Variance Ratio of Style PCA Components')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_explained_variance_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
