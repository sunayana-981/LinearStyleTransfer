{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from libs.models import encoder3,encoder4\n",
    "from libs.models import decoder3,decoder4\n",
    "import numpy as np\n",
    "from libs.Matrix import MulLayer\n",
    "from libs.Criterion import LossCriterion\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "class LossCriterion(nn.Module):\n",
    "    def __init__(self, style_layers, content_layers, style_weight, content_weight):\n",
    "        super(LossCriterion, self).__init__()\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.style_weight = style_weight\n",
    "        self.content_weight = content_weight\n",
    "        self.styleLosses = [styleLoss()] * len(style_layers)\n",
    "        self.contentLosses = [nn.MSELoss()] * len(content_layers)\n",
    "\n",
    "    def forward(self, tF, sF, cF):\n",
    "        # Content loss\n",
    "        totalContentLoss = 0\n",
    "        for i, layer in enumerate(self.content_layers):\n",
    "            cf_i = cF[layer].detach()\n",
    "            tf_i = tF[layer]\n",
    "            loss_i = self.contentLosses[i]\n",
    "            totalContentLoss += loss_i(tf_i, cf_i)\n",
    "        totalContentLoss = totalContentLoss * self.content_weight\n",
    "\n",
    "        # Style loss\n",
    "        totalStyleLoss = 0\n",
    "        for i, layer in enumerate(self.style_layers):\n",
    "            sf_i = sF[layer].detach()\n",
    "            tf_i = tF[layer]\n",
    "            loss_i = self.styleLosses[i]\n",
    "            totalStyleLoss += loss_i(tf_i, sf_i)\n",
    "        totalStyleLoss = totalStyleLoss * self.style_weight\n",
    "\n",
    "        loss = totalStyleLoss + totalContentLoss\n",
    "        return loss, totalStyleLoss, totalContentLoss\n",
    "\n",
    "\n",
    "class styleLoss(nn.Module):\n",
    "    def forward(self,input,target):\n",
    "        ib,ic,ih,iw = input.size()\n",
    "        iF = input.view(ib,ic,-1)\n",
    "        iMean = torch.mean(iF,dim=2)\n",
    "        iCov = GramMatrix()(input)\n",
    "\n",
    "        tb,tc,th,tw = target.size()\n",
    "        tF = target.view(tb,tc,-1)\n",
    "        tMean = torch.mean(tF,dim=2)\n",
    "        tCov = GramMatrix()(target)\n",
    "\n",
    "        loss = nn.MSELoss(size_average=False)(iMean,tMean) + nn.MSELoss(size_average=False)(iCov,tCov)\n",
    "        return loss/tb\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self,input):\n",
    "        b, c, h, w = input.size()\n",
    "        f = input.view(b,c,h*w) # bxcx(hxw)\n",
    "        # torch.bmm(batch1, batch2, out=None)   #\n",
    "        # batch1: bxmxp, batch2: bxpxn -> bxmxn #\n",
    "        G = torch.bmm(f,f.transpose(1,2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n",
    "        return G.div_(c*h*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from libs.Loader import Dataset\n",
    "from libs.models import encoder4, decoder4\n",
    "from libs.Criterion import LossCriterion\n",
    "\n",
    "\n",
    "\n",
    "class Loss_sensitivity:\n",
    "    def __init__(self, vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device):\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.dec = dec.to(device)\n",
    "        self.matrix = matrix.to(device)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.criterion = LossCriterion(style_layers, content_layers, style_weight, content_weight)\n",
    "        self.device = device\n",
    "\n",
    "    def add_noise(self, matrix, sigma):\n",
    "        noise = torch.randn_like(matrix) * sigma\n",
    "        return matrix + noise\n",
    "\n",
    "    def forward(self, contentV, styleV):\n",
    "        with torch.no_grad():\n",
    "            sF = self.vgg(styleV)\n",
    "            cF = self.vgg(contentV)\n",
    "        return sF, cF\n",
    "\n",
    "    def run_experiment(self, contentV, styleV, sigmas):\n",
    "        sigma_values = []\n",
    "        loss_values = []\n",
    "\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "\n",
    "        for sigma in sigmas:\n",
    "            \n",
    "            transformed_features, transmatrix = self.matrix(cF[self.style_layers[0]], sF[self.style_layers[0]])\n",
    "            \n",
    "            # Add noise to the transformation matrix\n",
    "            noisy_transmatrix = self.add_noise(transmatrix, sigma)\n",
    "            \n",
    "            # Apply the noisy transformation matrix\n",
    "            b, c, h, w = transformed_features.size()\n",
    "            compressed_features = self.matrix.compress(transformed_features)\n",
    "            noisy_transfeature = torch.bmm(noisy_transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            noisy_transfeature = noisy_transfeature.view(b, self.matrix.matrixSize, h, w)\n",
    "            noisy_transfeature = self.matrix.unzip(noisy_transfeature)\n",
    "            \n",
    "            noisy_transfer = self.dec(noisy_transfeature).clamp(0, 1)\n",
    "\n",
    "            tF = self.vgg(noisy_transfer)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "\n",
    "            sigma_values.append(sigma)\n",
    "            loss_values.append(total_loss.item())\n",
    "            print(f\"Sigma: {sigma}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "        return sigma_values, loss_values\n",
    "    \n",
    "    def run_scaling_vector_experiment(self, contentV, styleV, scale_factors, vector_magnitudes):\n",
    "        scale_values = []\n",
    "        vector_values = []\n",
    "        loss_values = []\n",
    "\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "\n",
    "        # Get the original transformation matrix\n",
    "        _, transmatrix = self.matrix(cF[self.style_layers[0]], sF[self.style_layers[0]])\n",
    "\n",
    "        for scale in scale_factors:\n",
    "            for vector_mag in vector_magnitudes:\n",
    "                # Scale the transformation matrix\n",
    "                scaled_matrix = transmatrix * scale\n",
    "\n",
    "                # Create a simple vector and add it to each row of the matrix\n",
    "                vector = torch.ones_like(scaled_matrix[0, 0]) * vector_mag\n",
    "                modified_matrix = scaled_matrix + vector.unsqueeze(1)\n",
    "\n",
    "                # Apply the modified transformation\n",
    "                b, c, h, w = cF[self.style_layers[0]].size()\n",
    "                compressed_features = self.matrix.compress(cF[self.style_layers[0]])\n",
    "                modified_features = torch.bmm(modified_matrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "                modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "                modified_features = self.matrix.unzip(modified_features)\n",
    "\n",
    "                modified_transfer = self.dec(modified_features).clamp(0, 1)\n",
    "\n",
    "                tF = self.vgg(modified_transfer)\n",
    "                total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "\n",
    "                scale_values.append(scale)\n",
    "                vector_values.append(vector_mag)\n",
    "                loss_values.append(total_loss.item())\n",
    "                print(f\"Scale: {scale}, Vector Magnitude: {vector_mag}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "        return scale_values, vector_values, loss_values\n",
    "    \n",
    "    def run_dimension_dropping_experiment(self, contentV, styleV):\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "        \n",
    "        # Get the original features\n",
    "        content_features = cF[self.style_layers[0]]\n",
    "        style_features = sF[self.style_layers[0]]\n",
    "        \n",
    "        b, c, h, w = content_features.size()\n",
    "        \n",
    "        content_loss_impacts = []\n",
    "        style_loss_impacts = []\n",
    "        \n",
    "        # Calculate baseline loss\n",
    "        _, transmatrix = self.matrix(content_features, style_features)\n",
    "        compressed_features = self.matrix.compress(content_features)\n",
    "        modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "        modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "        modified_features = self.matrix.unzip(modified_features)\n",
    "        stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "        tF = self.vgg(stylized_image)\n",
    "        baseline_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "        \n",
    "        # Analyze content dimensions\n",
    "        for dim in range(c):\n",
    "            dropped_content = content_features.clone()\n",
    "            dropped_content[:, dim, :, :] = 0\n",
    "            \n",
    "            # Apply the transformation\n",
    "            _, transmatrix = self.matrix(dropped_content, style_features)\n",
    "            compressed_features = self.matrix.compress(dropped_content)\n",
    "            modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "            modified_features = self.matrix.unzip(modified_features)\n",
    "            \n",
    "            # Generate the stylized image and calculate loss\n",
    "            stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "            tF = self.vgg(stylized_image)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            \n",
    "            content_loss_impacts.append(total_loss.item() - baseline_loss.item())\n",
    "        \n",
    "        # Analyze style dimensions\n",
    "        for dim in range(c):\n",
    "            dropped_style = style_features.clone()\n",
    "            dropped_style[:, dim, :, :] = 0\n",
    "            \n",
    "            # Apply the transformation\n",
    "            _, transmatrix = self.matrix(content_features, dropped_style)\n",
    "            compressed_features = self.matrix.compress(content_features)\n",
    "            modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "            modified_features = self.matrix.unzip(modified_features)\n",
    "            \n",
    "            # Generate the stylized image and calculate loss\n",
    "            stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "            tF = self.vgg(stylized_image)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            \n",
    "            style_loss_impacts.append(total_loss.item() - baseline_loss.item())\n",
    "        \n",
    "        return content_loss_impacts, style_loss_impacts\n",
    "    \n",
    "    def run_pca_experiment(self, contentV, styleV, n_components=50):\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "        \n",
    "        # Get the original features\n",
    "        content_features = cF[self.style_layers[0]]\n",
    "        style_features = sF[self.style_layers[0]]\n",
    "        \n",
    "        b, c, h, w = content_features.size()\n",
    "        \n",
    "        # Reshape features for PCA\n",
    "        content_features_flat = content_features.view(b, c, -1).permute(0, 2, 1).reshape(-1, c)\n",
    "        style_features_flat = style_features.view(b, c, -1).permute(0, 2, 1).reshape(-1, c)\n",
    "        \n",
    "        # Apply PCA\n",
    "        content_pca = PCA(n_components=n_components)\n",
    "        style_pca = PCA(n_components=n_components)\n",
    "        \n",
    "        content_pca.fit(content_features_flat.cpu().numpy())\n",
    "        style_pca.fit(style_features_flat.cpu().numpy())\n",
    "        \n",
    "        # Calculate baseline loss\n",
    "        _, transmatrix = self.matrix(content_features, style_features)\n",
    "        compressed_features = self.matrix.compress(content_features)\n",
    "        modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "        modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "        modified_features = self.matrix.unzip(modified_features)\n",
    "        stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "        tF = self.vgg(stylized_image)\n",
    "        baseline_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "        \n",
    "        content_pca_loss_impacts = []\n",
    "        style_pca_loss_impacts = []\n",
    "        \n",
    "        # Analyze PCA components for content\n",
    "        for i in range(n_components):\n",
    "            pca_component = torch.from_numpy(content_pca.components_[i]).float().to(self.device)\n",
    "            content_flat = content_features.view(b, c, -1)\n",
    "            projection = torch.matmul(content_flat.permute(0, 2, 1), pca_component.unsqueeze(1)).permute(0, 2, 1)\n",
    "            dropped_content_flat = content_flat - projection * pca_component.unsqueeze(0).unsqueeze(-1)\n",
    "            dropped_content = dropped_content_flat.view(b, c, h, w)\n",
    "            \n",
    "            # Apply the transformation\n",
    "            _, transmatrix = self.matrix(dropped_content, style_features)\n",
    "            compressed_features = self.matrix.compress(dropped_content)\n",
    "            modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "            modified_features = self.matrix.unzip(modified_features)\n",
    "            \n",
    "            # Generate the stylized image and calculate loss\n",
    "            stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "            tF = self.vgg(stylized_image)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            \n",
    "            content_pca_loss_impacts.append(total_loss.item() - baseline_loss.item())\n",
    "        \n",
    "        # Analyze PCA components for style (similar changes as content)\n",
    "        for i in range(n_components):\n",
    "            pca_component = torch.from_numpy(style_pca.components_[i]).float().to(self.device)\n",
    "            style_flat = style_features.view(b, c, -1)\n",
    "            projection = torch.matmul(style_flat.permute(0, 2, 1), pca_component.unsqueeze(1)).permute(0, 2, 1)\n",
    "            dropped_style_flat = style_flat - projection * pca_component.unsqueeze(0).unsqueeze(-1)\n",
    "            dropped_style = dropped_style_flat.view(b, c, h, w)\n",
    "            \n",
    "            # Apply the transformation\n",
    "            _, transmatrix = self.matrix(content_features, dropped_style)\n",
    "            compressed_features = self.matrix.compress(content_features)\n",
    "            modified_features = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "            modified_features = modified_features.view(b, self.matrix.matrixSize, h, w)\n",
    "            modified_features = self.matrix.unzip(modified_features)\n",
    "            \n",
    "            # Generate the stylized image and calculate loss\n",
    "            stylized_image = self.dec(modified_features).clamp(0, 1)\n",
    "            tF = self.vgg(stylized_image)\n",
    "            total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "            \n",
    "            style_pca_loss_impacts.append(total_loss.item() - baseline_loss.item())\n",
    "        \n",
    "        return content_pca_loss_impacts, style_pca_loss_impacts, content_pca, style_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load models\n",
    "    vgg = encoder4()\n",
    "    dec = decoder4()\n",
    "    matrix = MulLayer('r41')\n",
    "    \n",
    "    vgg_dir = 'models/vgg_r41.pth'\n",
    "    decoder_dir = \"models/dec_r41.pth\"\n",
    "    \n",
    "    vgg.load_state_dict(torch.load(vgg_dir))\n",
    "    dec.load_state_dict(torch.load(decoder_dir))\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "    \n",
    "    class Options:\n",
    "        def __init__(self):\n",
    "            self.contentPath = \"data/content/\"\n",
    "            self.stylePath = \"data/style/\"\n",
    "            self.loadSize = 256\n",
    "            self.fineSize = 256\n",
    "\n",
    "    opt = Options()\n",
    "    \n",
    "    loss_sensitivity = Loss_sensitivity(vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device)\n",
    "    \n",
    "    # Load a single content and style image for the experiment\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    \n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    \n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run experiments on sigma\n",
    "    sigmas = np.linspace(0, 0.2, 10)\n",
    "    sigma_values, loss_values = loss_sensitivity.run_experiment(contentV, styleV, sigmas)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sigma_values, loss_values, '-o')\n",
    "    plt.xlabel('Sigma (Noise Level)')\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.title('Noise Sensitivity of Transformation Matrix')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('noise_sensitivity_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load models\n",
    "    vgg = encoder4()\n",
    "    dec = decoder4()\n",
    "    matrix = MulLayer('r41')\n",
    "    \n",
    "    vgg_dir = 'models/vgg_r41.pth'\n",
    "    decoder_dir = \"models/dec_r41.pth\"\n",
    "    \n",
    "    vgg.load_state_dict(torch.load(vgg_dir))\n",
    "    dec.load_state_dict(torch.load(decoder_dir))\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "    \n",
    "    class Options:\n",
    "        def __init__(self):\n",
    "            self.contentPath = \"data/content/\"\n",
    "            self.stylePath = \"data/style/\"\n",
    "            self.loadSize = 256\n",
    "            self.fineSize = 256\n",
    "\n",
    "    opt = Options()\n",
    "    \n",
    "    loss_sensitivity = Loss_sensitivity(vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device)\n",
    "    \n",
    "    # Load a single content and style image for the experiment\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    \n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    \n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run scaling and vector addition experiment\n",
    "    scale_factors = np.linspace(0.5, 1.5, 5)\n",
    "    vector_magnitudes = np.linspace(0, 0.1, 5)\n",
    "    scale_values, vector_values, loss_values = loss_sensitivity.run_scaling_vector_experiment(contentV, styleV, scale_factors, vector_magnitudes)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(scale_values, vector_values, loss_values, c=loss_values, cmap='viridis')\n",
    "    ax.set_xlabel('Scale Factor')\n",
    "    ax.set_ylabel('Vector Magnitude')\n",
    "    ax.set_zlabel('Total Loss')\n",
    "    ax.set_title('Loss Sensitivity to Scaling and Vector Addition')\n",
    "    fig.colorbar(scatter, label='Total Loss')\n",
    "    plt.savefig('scaling_vector_sensitivity_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load models\n",
    "    vgg = encoder4()\n",
    "    dec = decoder4()\n",
    "    matrix = MulLayer('r41')\n",
    "    \n",
    "    vgg_dir = 'models/vgg_r41.pth'\n",
    "    decoder_dir = \"models/dec_r41.pth\"\n",
    "    \n",
    "    vgg.load_state_dict(torch.load(vgg_dir))\n",
    "    dec.load_state_dict(torch.load(decoder_dir))\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "    \n",
    "    class Options:\n",
    "        def __init__(self):\n",
    "            self.contentPath = \"data/content/\"\n",
    "            self.stylePath = \"data/style/\"\n",
    "            self.loadSize = 256\n",
    "            self.fineSize = 256\n",
    "\n",
    "    opt = Options()\n",
    "    \n",
    "    loss_sensitivity = Loss_sensitivity(vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device)\n",
    "    \n",
    "    # Load a single content and style image for the experiment\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    \n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    \n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run dimension dropping experiment\n",
    "    content_loss_impacts, style_loss_impacts = loss_sensitivity.run_dimension_dropping_experiment(contentV, styleV)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 16))\n",
    "    \n",
    "    ax1.plot(range(len(content_loss_impacts)), content_loss_impacts)\n",
    "    ax1.set_xlabel('Content Feature Dimension')\n",
    "    ax1.set_ylabel('Change in Total Loss')\n",
    "    ax1.set_title('Impact of Dropping Individual Content Feature Dimensions on Total Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(range(len(style_loss_impacts)), style_loss_impacts)\n",
    "    ax2.set_xlabel('Style Feature Dimension')\n",
    "    ax2.set_ylabel('Change in Total Loss')\n",
    "    ax2.set_title('Impact of Dropping Individual Style Feature Dimensions on Total Loss')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dimension_dropping_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ... (previous setup code remains unchanged) ...\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load models\n",
    "    vgg = encoder4()\n",
    "    dec = decoder4()\n",
    "    matrix = MulLayer('r41')\n",
    "    \n",
    "    vgg_dir = 'models/vgg_r41.pth'\n",
    "    decoder_dir = \"models/dec_r41.pth\"\n",
    "    \n",
    "    vgg.load_state_dict(torch.load(vgg_dir))\n",
    "    dec.load_state_dict(torch.load(decoder_dir))\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "    \n",
    "    class Options:\n",
    "        def __init__(self):\n",
    "            self.contentPath = \"data/content/\"\n",
    "            self.stylePath = \"data/style/\"\n",
    "            self.loadSize = 256\n",
    "            self.fineSize = 256\n",
    "\n",
    "    opt = Options()\n",
    "    loss_sensitivity = Loss_sensitivity(vgg, dec, matrix, style_layers, content_layers, style_weight, content_weight, device)\n",
    "    \n",
    "    # Load a single content and style image for the experiment\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    \n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    \n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Run dimension dropping experiment\n",
    "    content_loss_impacts, style_loss_impacts = loss_sensitivity.run_dimension_dropping_experiment(contentV, styleV)\n",
    "    \n",
    "    # Run PCA experiment\n",
    "    n_components = 50\n",
    "    content_pca_loss_impacts, style_pca_loss_impacts, content_pca, style_pca = loss_sensitivity.run_pca_experiment(contentV, styleV, n_components)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 20))\n",
    "    \n",
    "    ax1.plot(range(len(content_loss_impacts)), content_loss_impacts)\n",
    "    ax1.set_xlabel('Content Feature Dimension')\n",
    "    ax1.set_ylabel('Change in Total Loss')\n",
    "    ax1.set_title('Impact of Dropping Individual Content Feature Dimensions')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(range(len(style_loss_impacts)), style_loss_impacts)\n",
    "    ax2.set_xlabel('Style Feature Dimension')\n",
    "    ax2.set_ylabel('Change in Total Loss')\n",
    "    ax2.set_title('Impact of Dropping Individual Style Feature Dimensions')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    ax3.plot(range(n_components), content_pca_loss_impacts)\n",
    "    ax3.set_xlabel('Content PCA Component')\n",
    "    ax3.set_ylabel('Change in Total Loss')\n",
    "    ax3.set_title('Impact of Dropping Content PCA Components')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    ax4.plot(range(n_components), style_pca_loss_impacts)\n",
    "    ax4.set_xlabel('Style PCA Component')\n",
    "    ax4.set_ylabel('Change in Total Loss')\n",
    "    ax4.set_title('Impact of Dropping Style PCA Components')\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dimension_and_pca_analysis_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    ax1.plot(range(1, n_components + 1), content_pca.explained_variance_ratio_)\n",
    "    ax1.set_xlabel('Number of Components')\n",
    "    ax1.set_ylabel('Explained Variance Ratio')\n",
    "    ax1.set_title('Explained Variance Ratio of Content PCA Components')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(range(1, n_components + 1), style_pca.explained_variance_ratio_)\n",
    "    ax2.set_xlabel('Number of Components')\n",
    "    ax2.set_ylabel('Explained Variance Ratio')\n",
    "    ax2.set_title('Explained Variance Ratio of Style PCA Components')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_explained_variance_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
