{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from libs.models import encoder3,encoder4\n",
    "from libs.models import decoder3,decoder4\n",
    "import numpy as np\n",
    "from libs.Matrix import MulLayer\n",
    "from libs.Criterion import LossCriterion\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "class LossCriterion(nn.Module):\n",
    "    def __init__(self, style_layers, content_layers, style_weight, content_weight):\n",
    "        super(LossCriterion, self).__init__()\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.style_weight = style_weight\n",
    "        self.content_weight = content_weight\n",
    "        self.styleLosses = [styleLoss()] * len(style_layers)\n",
    "        self.contentLosses = [nn.MSELoss()] * len(content_layers)\n",
    "\n",
    "    def forward(self, tF, sF, cF):\n",
    "        # Content loss\n",
    "        totalContentLoss = 0\n",
    "        for i, layer in enumerate(self.content_layers):\n",
    "            cf_i = cF[layer].detach()\n",
    "            tf_i = tF[layer]\n",
    "            loss_i = self.contentLosses[i]\n",
    "            totalContentLoss += loss_i(tf_i, cf_i)\n",
    "        totalContentLoss = totalContentLoss * self.content_weight\n",
    "\n",
    "        # Style loss\n",
    "        \n",
    "        totalStyleLoss = 0\n",
    "        for i, layer in enumerate(self.style_layers):\n",
    "            sf_i = sF[layer].detach()\n",
    "            tf_i = tF[layer]\n",
    "            loss_i = self.styleLosses[i]\n",
    "            totalStyleLoss += loss_i(tf_i, sf_i)\n",
    "        totalStyleLoss = totalStyleLoss * self.style_weight\n",
    "\n",
    "        loss = totalStyleLoss + totalContentLoss\n",
    "        return loss, totalStyleLoss, totalContentLoss\n",
    "\n",
    "\n",
    "class styleLoss(nn.Module):\n",
    "    def forward(self,input,target):\n",
    "        ib,ic,ih,iw = input.size()\n",
    "        iF = input.view(ib,ic,-1)\n",
    "        iMean = torch.mean(iF,dim=2)\n",
    "        iCov = GramMatrix()(input)\n",
    "\n",
    "        tb,tc,th,tw = target.size()\n",
    "        tF = target.view(tb,tc,-1)\n",
    "        tMean = torch.mean(tF,dim=2)\n",
    "        tCov = GramMatrix()(target)\n",
    "\n",
    "        loss = nn.MSELoss(size_average=False)(iMean,tMean) + nn.MSELoss(size_average=False)(iCov,tCov)\n",
    "        return loss/tb\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self,input):\n",
    "        b, c, h, w = input.size()\n",
    "        f = input.view(b,c,h*w) # bxcx(hxw)\n",
    "        # torch.bmm(batch1, batch2, out=None)   #\n",
    "        # batch1: bxmxp, batch2: bxpxn -> bxmxn #\n",
    "        G = torch.bmm(f,f.transpose(1,2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n",
    "        return G.div_(c*h*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from libs.Loader import Dataset\n",
    "from libs.models import encoder4, decoder4\n",
    "from libs.Criterion import LossCriterion\n",
    "from libs.Matrix import MulLayer\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    " \n",
    "class LossSensitivity:\n",
    "    def __init__(self, vgg: nn.Module, dec: nn.Module, matrix: MulLayer,\n",
    "                 style_layers: List[str], content_layers: List[str],\n",
    "                 style_weight: float, content_weight: float, device: torch.device):\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.dec = dec.to(device)\n",
    "        self.matrix = matrix.to(device)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.criterion = LossCriterion(style_layers, content_layers, style_weight, content_weight)\n",
    "        self.device = device\n",
    " \n",
    "    def add_noise(self, matrix: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "        return matrix + torch.randn_like(matrix) * sigma\n",
    " \n",
    "    @torch.no_grad()\n",
    "    def forward(self, contentV: torch.Tensor, styleV: torch.Tensor) -> Tuple[dict, dict]:\n",
    "        return self.vgg(styleV), self.vgg(contentV)\n",
    " \n",
    "    def compute_loss(self, contentV: torch.Tensor, styleV: torch.Tensor, noisy_matrix: torch.Tensor) -> float:\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "        \n",
    "        transformed_features, _ = self.matrix(cF[self.style_layers[0]], sF[self.style_layers[0]])\n",
    "        b, c, h, w = transformed_features.size()\n",
    "        compressed_features = self.matrix.compress(transformed_features)\n",
    "        \n",
    "        if noisy_matrix.size(1) != compressed_features.view(b, self.matrix.matrixSize, -1).size(1):\n",
    "            print(f\"Dimension mismatch: {noisy_matrix.size()} vs {compressed_features.size()}\")\n",
    "            return float('inf')\n",
    "        \n",
    "        noisy_transfeature = torch.bmm(noisy_matrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "        noisy_transfeature = noisy_transfeature.view(b, self.matrix.matrixSize, h, w)\n",
    "        noisy_transfeature = self.matrix.unzip(noisy_transfeature)\n",
    "        \n",
    "        noisy_transfer = self.dec(noisy_transfeature).clamp(0, 1)\n",
    "        tF = self.vgg(noisy_transfer)\n",
    "        \n",
    "        total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "        return total_loss.item()\n",
    " \n",
    "    def compute_matrix_metrics(self, original_matrix: torch.Tensor, noisy_matrix: torch.Tensor) -> Tuple[float, Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # Frobenius Norm Difference\n",
    "        frobenius_diff = torch.norm(original_matrix - noisy_matrix, p='fro').item()\n",
    "        \n",
    "        # Eigenvalue/Eigenvector Changes\n",
    "        orig_eigenvalues, orig_eigenvectors = torch.linalg.eig(original_matrix)\n",
    "        noisy_eigenvalues, noisy_eigenvectors = torch.linalg.eig(noisy_matrix)\n",
    "        \n",
    "        return frobenius_diff, (orig_eigenvalues, noisy_eigenvalues), (orig_eigenvectors, noisy_eigenvectors)\n",
    " \n",
    "    def run_experiment(self, contentV: torch.Tensor, styleV: torch.Tensor,\n",
    "                       sigmas: np.ndarray, matrix: torch.Tensor) -> Tuple[List[float], List[float], List[float], List[Tuple[torch.Tensor, torch.Tensor]], List[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        sigma_values = []\n",
    "        loss_values = []\n",
    "        frobenius_diffs = []\n",
    "        eigenvalue_changes = []\n",
    "        eigenvector_changes = []\n",
    " \n",
    "        for sigma in sigmas:\n",
    "            noisy_matrix = self.add_noise(matrix, sigma)\n",
    "            loss = self.compute_loss(contentV, styleV, noisy_matrix)\n",
    "            if loss == float('inf'):\n",
    "                print(f\"Skipping sigma {sigma} due to dimension mismatch.\")\n",
    "                continue\n",
    "            \n",
    "            frobenius_diff, eigenvalues, eigenvectors = self.compute_matrix_metrics(matrix, noisy_matrix)\n",
    "            \n",
    "            sigma_values.append(sigma)\n",
    "            loss_values.append(loss)\n",
    "            frobenius_diffs.append(frobenius_diff)\n",
    "            eigenvalue_changes.append(eigenvalues)\n",
    "            eigenvector_changes.append(eigenvectors)\n",
    " \n",
    "        return sigma_values, loss_values, frobenius_diffs, eigenvalue_changes, eigenvector_changes\n",
    " \n",
    "\n",
    "def process_style_dir(style_dir: str, opt, loss_sensitivity: LossSensitivity,\n",
    "                      sigmas: np.ndarray, device: torch.device) -> Tuple[List[List[float]], List[List[float]], List[List[Tuple[torch.Tensor, torch.Tensor]]], List[List[Tuple[torch.Tensor, torch.Tensor]]]]:\n",
    "    style_path = os.path.join(opt.matrixPath, style_dir)\n",
    "    matrix_files = [f for f in os.listdir(style_path) if f.endswith('.pth')]\n",
    "    all_loss_values = []\n",
    "    all_frobenius_diffs = []\n",
    "    all_eigenvalue_changes = []\n",
    "    all_eigenvector_changes = []\n",
    "\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "\n",
    "    for matrix_file in tqdm(matrix_files, desc=f\"Processing {style_dir}\"):\n",
    "        matrix_path = os.path.join(style_path, matrix_file)\n",
    "        saved_matrix = torch.load(matrix_path, map_location=device)\n",
    "        _, loss_values, frobenius_diffs, eigenvalue_changes, eigenvector_changes = loss_sensitivity.run_experiment(contentV, styleV, sigmas, saved_matrix)\n",
    "        all_loss_values.append(loss_values)\n",
    "        all_frobenius_diffs.append(frobenius_diffs)\n",
    "        all_eigenvalue_changes.append(eigenvalue_changes)\n",
    "        all_eigenvector_changes.append(eigenvector_changes)\n",
    "\n",
    "    return all_loss_values, all_frobenius_diffs, all_eigenvalue_changes, all_eigenvector_changes\n",
    "\n",
    "def plot_style_results(style_dir: str, sigmas: np.ndarray, avg_loss_values: np.ndarray, avg_frobenius_diffs: np.ndarray):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    ax1.plot(sigmas[:len(avg_loss_values)], avg_loss_values, '-o')\n",
    "    ax1.set_xlabel('Sigma (Noise Level)')\n",
    "    ax1.set_ylabel('Average Total Loss')\n",
    "    ax1.set_title(f'Average Noise Sensitivity for Style: {style_dir}')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(sigmas[:len(avg_frobenius_diffs)], avg_frobenius_diffs, '-o')\n",
    "    ax2.set_xlabel('Sigma (Noise Level)')\n",
    "    ax2.set_ylabel('Average Frobenius Norm Difference')\n",
    "    ax2.set_title(f'Average Matrix Change for Style: {style_dir}')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'noise_sensitivity_{style_dir}.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_eigenvalue_changes(style_dir: str, sigmas: np.ndarray, eigenvalue_changes: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "    avg_eigenvalue_diffs = []\n",
    "    for orig, noisy in eigenvalue_changes:\n",
    "        avg_diff = torch.mean(torch.abs(orig - noisy)).item()\n",
    "        avg_eigenvalue_diffs.append(avg_diff)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sigmas, avg_eigenvalue_diffs, '-o')\n",
    "    plt.xlabel('Sigma (Noise Level)')\n",
    "    plt.ylabel('Average Eigenvalue Difference')\n",
    "    plt.title(f'Eigenvalue Sensitivity for Style: {style_dir}')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'eigenvalue_sensitivity_{style_dir}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_average_frobenius_norm_trend(sigmas: np.ndarray, all_styles_frobenius_diffs: List[np.ndarray]):\n",
    "    avg_frobenius_diffs = np.mean(all_styles_frobenius_diffs, axis=0)\n",
    "    std_frobenius_diffs = np.std(all_styles_frobenius_diffs, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sigmas, avg_frobenius_diffs, '-', color='blue', label='Average')\n",
    "    plt.fill_between(sigmas,\n",
    "                     avg_frobenius_diffs - std_frobenius_diffs,\n",
    "                     avg_frobenius_diffs + std_frobenius_diffs,\n",
    "                     alpha=0.3, color='lightblue', label='±1 Standard Deviation')\n",
    "\n",
    "    plt.plot(sigmas, avg_frobenius_diffs - std_frobenius_diffs, '--', color='red', alpha=0.5, label='-1 Std Dev')\n",
    "    plt.plot(sigmas, avg_frobenius_diffs + std_frobenius_diffs, '--', color='red', alpha=0.5, label='+1 Std Dev')\n",
    "    \n",
    "    plt.xlabel('Sigma (Noise Level)')\n",
    "    plt.ylabel('Average Frobenius Norm Difference')\n",
    "    plt.title('Average Frobenius Norm Trend Across All Styles')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('average_frobenius_norm_trend.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Maximum average Frobenius norm difference: {np.max(avg_frobenius_diffs):.4f}\")\n",
    "    print(f\"Maximum standard deviation: {np.max(std_frobenius_diffs):.4f}\")\n",
    "    print(f\"Sigma at maximum average difference: {sigmas[np.argmax(avg_frobenius_diffs)]:.4f}\")\n",
    "\n",
    "def identify_sensitive_components(style_dir: str, sigmas: np.ndarray, eigenvalue_changes: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "    num_eigenvalues = eigenvalue_changes[0][0].shape[0]\n",
    "    eigenvalue_sensitivities = [[] for _ in range(num_eigenvalues)]\n",
    "    \n",
    "    for orig, noisy in eigenvalue_changes:\n",
    "        diffs = torch.abs(orig - noisy)\n",
    "        for i in range(num_eigenvalues):\n",
    "            eigenvalue_sensitivities[i].append(diffs[i].item())\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(num_eigenvalues):\n",
    "        plt.plot(sigmas, eigenvalue_sensitivities[i], label=f'Eigenvalue {i+1}')\n",
    "    plt.xlabel('Sigma (Noise Level)')\n",
    "    plt.ylabel('Eigenvalue Difference')\n",
    "    plt.title(f'Individual Eigenvalue Sensitivities for Style: {style_dir}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'individual_eigenvalue_sensitivities_{style_dir}.png')\n",
    "    plt.close()\n",
    "\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.contentPath = \"data/content/\"\n",
    "        self.stylePath = \"data/style/\"\n",
    "        self.loadSize = 256\n",
    "        self.fineSize = 256\n",
    "        self.matrixPath = \"Matrices/\"\n",
    "\n",
    "def load_models(device: torch.device) -> Tuple[nn.Module, nn.Module, MulLayer]:\n",
    "    vgg = encoder4()\n",
    "    dec = decoder4()\n",
    "    matrix = MulLayer('r41')\n",
    "    vgg.load_state_dict(torch.load('models/vgg_r41.pth', map_location=device))\n",
    "    dec.load_state_dict(torch.load('models/dec_r41.pth', map_location=device))\n",
    "    return vgg, dec, matrix\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vgg, dec, matrix = load_models(device)\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "\n",
    "    opt = Options()\n",
    "    loss_sensitivity = LossSensitivity(vgg, dec, matrix, style_layers, content_layers,\n",
    "                                       style_weight, content_weight, device)\n",
    "\n",
    "    sigmas = np.linspace(0, 0.2, 100)\n",
    "    style_dirs = [d for d in os.listdir(opt.matrixPath) if os.path.isdir(os.path.join(opt.matrixPath, d))]\n",
    "    \n",
    "    all_styles_frobenius_diffs = []\n",
    "\n",
    "    for style_dir in style_dirs:\n",
    "        try:\n",
    "            all_loss_values, all_frobenius_diffs, all_eigenvalue_changes, all_eigenvector_changes = process_style_dir(style_dir, opt, loss_sensitivity, sigmas, device)\n",
    "            \n",
    "            if all_loss_values:\n",
    "                avg_loss_values = np.mean(all_loss_values, axis=0)\n",
    "                avg_frobenius_diffs = np.mean(all_frobenius_diffs, axis=0)\n",
    "                all_styles_frobenius_diffs.append(avg_frobenius_diffs)\n",
    "                plot_style_results(style_dir, sigmas, avg_loss_values, avg_frobenius_diffs)\n",
    "                analyze_eigenvalue_changes(style_dir, sigmas, all_eigenvalue_changes[0])  # Analyze first set of eigenvalue changes\n",
    "                identify_sensitive_components(style_dir, sigmas, all_eigenvalue_changes[0])\n",
    "            else:\n",
    "                print(f\"Warning: No data available for style directory {style_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing style directory {style_dir}: {str(e)}\")\n",
    "\n",
    "    if all_styles_frobenius_diffs:\n",
    "        plot_average_frobenius_norm_trend(sigmas, all_styles_frobenius_diffs)\n",
    "    else:\n",
    "        print(\"Warning: No Frobenius norm data available for analysis across styles.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class LossSensitivity:\n",
    "    def __init__(self, vgg: nn.Module, dec: nn.Module, matrix: MulLayer, \n",
    "                 style_layers: List[str], content_layers: List[str], \n",
    "                 style_weight: float, content_weight: float, device: torch.device):\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.dec = dec.to(device)\n",
    "        self.matrix = matrix.to(device)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.criterion = LossCriterion(style_layers, content_layers, style_weight, content_weight)\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, contentV: torch.Tensor, styleV: torch.Tensor) -> Tuple[dict, dict]:\n",
    "        return self.vgg(styleV), self.vgg(contentV)\n",
    "\n",
    "    def compute_loss(self, contentV: torch.Tensor, styleV: torch.Tensor, transmatrix: torch.Tensor) -> float:\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "        \n",
    "        transformed_features, _ = self.matrix(cF[self.style_layers[0]], sF[self.style_layers[0]])\n",
    "        b, c, h, w = transformed_features.size()\n",
    "        compressed_features = self.matrix.compress(transformed_features)\n",
    "        \n",
    "        transfeature = torch.bmm(transmatrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "        transfeature = transfeature.view(b, self.matrix.matrixSize, h, w)\n",
    "        transfeature = self.matrix.unzip(transfeature)\n",
    "        \n",
    "        transfer = self.dec(transfeature).clamp(0, 1)\n",
    "        tF = self.vgg(transfer)\n",
    "        \n",
    "        total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "        return total_loss.item()\n",
    "\n",
    "    def run_dimension_dropping_experiment(self, contentV: torch.Tensor, styleV: torch.Tensor) -> Tuple[List[float], List[float]]:\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "        \n",
    "        content_features = cF[self.style_layers[0]]\n",
    "        style_features = sF[self.style_layers[0]]\n",
    "        \n",
    "        b, c, h, w = content_features.size()\n",
    "        \n",
    "        content_loss_impacts = []\n",
    "        style_loss_impacts = []\n",
    "        \n",
    "        # Calculate baseline loss\n",
    "        _, baseline_matrix = self.matrix(content_features, style_features)\n",
    "        baseline_loss = self.compute_loss(contentV, styleV, baseline_matrix)\n",
    "        \n",
    "        # Analyze content dimensions\n",
    "        for dim in tqdm(range(c), desc=\"Analyzing content dimensions\"):\n",
    "            dropped_content = content_features.clone()\n",
    "            dropped_content[:, dim, :, :] = 0\n",
    "            \n",
    "            _, transmatrix = self.matrix(dropped_content, style_features)\n",
    "            loss = self.compute_loss(contentV, styleV, transmatrix)\n",
    "            content_loss_impacts.append(loss - baseline_loss)\n",
    "        \n",
    "        # Analyze style dimensions\n",
    "        for dim in tqdm(range(c), desc=\"Analyzing style dimensions\"):\n",
    "            dropped_style = style_features.clone()\n",
    "            dropped_style[:, dim, :, :] = 0\n",
    "            \n",
    "            _, transmatrix = self.matrix(content_features, dropped_style)\n",
    "            loss = self.compute_loss(contentV, styleV, transmatrix)\n",
    "            style_loss_impacts.append(loss - baseline_loss)\n",
    "        \n",
    "        return content_loss_impacts, style_loss_impacts \n",
    "\n",
    "def plot_dimension_dropping_results(content_loss_impacts: List[float], style_loss_impacts: List[float], style_name: str):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(content_loss_impacts)), content_loss_impacts, '-o', label='Content')\n",
    "    plt.plot(range(len(style_loss_impacts)), style_loss_impacts, '-o', label='Style')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Loss Impact')\n",
    "    plt.title(f'Dimension Dropping Impact for Style: {style_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'dimension_dropping_impact_{style_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_comparison_dimension_dropping(style_dirs: List[str], all_content_loss_impacts: List[List[float]], all_style_loss_impacts: List[List[float]]):\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for style_dir, content_loss_impacts, style_loss_impacts in zip(style_dirs, all_content_loss_impacts, all_style_loss_impacts):\n",
    "        plt.plot(range(len(content_loss_impacts)), content_loss_impacts, '-', label=f'Content: {style_dir}')\n",
    "        plt.plot(range(len(style_loss_impacts)), style_loss_impacts, '--', label=f'Style: {style_dir}')\n",
    "\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Loss Impact')\n",
    "    plt.title('Dimension Dropping Impact Across Styles')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('comparison_dimension_dropping_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_overall_average_dimension_dropping(style_dirs: List[str], all_content_loss_impacts: List[List[float]], all_style_loss_impacts: List[List[float]]):\n",
    "    content_avg_loss = np.mean(all_content_loss_impacts, axis=0)\n",
    "    content_variance = np.var(all_content_loss_impacts, axis=0)\n",
    "    style_avg_loss = np.mean(all_style_loss_impacts, axis=0)\n",
    "    style_variance = np.var(all_style_loss_impacts, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(len(content_avg_loss)), content_avg_loss, '-', label='Average Content Impact')\n",
    "    plt.fill_between(range(len(content_avg_loss)), content_avg_loss - content_variance, content_avg_loss + content_variance, alpha=0.2, label='Content Variance')\n",
    "    plt.plot(range(len(style_avg_loss)), style_avg_loss, '-', label='Average Style Impact')\n",
    "    plt.fill_between(range(len(style_avg_loss)), style_avg_loss - style_variance, style_avg_loss + style_variance, alpha=0.2, label='Style Variance')\n",
    "    plt.xlabel('Dimension')\n",
    "    plt.ylabel('Average Loss Impact')\n",
    "    plt.title('Overall Average Dimension Dropping Impact Across Styles')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('overall_dimension_dropping_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_clustered_styles_dimension_dropping(style_dirs: List[str], all_content_loss_impacts: List[List[float]], all_style_loss_impacts: List[List[float]], n_clusters: int = 3):\n",
    "    combined_impacts = [np.concatenate([content, style]) for content, style in zip(all_content_loss_impacts, all_style_loss_impacts)]\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(combined_impacts)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    for cluster in set(labels):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for idx, style_dir in enumerate(style_dirs):\n",
    "            if labels[idx] == cluster:\n",
    "                plt.plot(range(len(all_content_loss_impacts[idx])), all_content_loss_impacts[idx], '-', label=f'Content: {style_dir}')\n",
    "                plt.plot(range(len(all_style_loss_impacts[idx])), all_style_loss_impacts[idx], '--', label=f'Style: {style_dir}')\n",
    "        plt.xlabel('Dimension')\n",
    "        plt.ylabel('Loss Impact')\n",
    "        plt.title(f'Dimension Dropping Impact for Cluster {cluster}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'cluster_{cluster}_dimension_dropping_plot.png')\n",
    "        plt.show()\n",
    "\n",
    "def process_style_dir_dimension_dropping(style_dir: str, opt: Options, loss_sensitivity: LossSensitivity, device: torch.device) -> Tuple[List[float], List[float]]:\n",
    "    style_path = os.path.join(opt.matrixPath, style_dir)\n",
    "    matrix_files = [f for f in os.listdir(style_path) if f.endswith('.pth')]\n",
    "    all_content_loss_impacts = []\n",
    "    all_style_loss_impacts = []\n",
    "\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "    contentV, _ = content_dataset[0]\n",
    "    styleV, _ = style_dataset[0]\n",
    "    contentV = contentV.unsqueeze(0).to(device)\n",
    "    styleV = styleV.unsqueeze(0).to(device)\n",
    "\n",
    "    for matrix_file in tqdm(matrix_files, desc=f\"Processing {style_dir}\"):\n",
    "        matrix_path = os.path.join(style_path, matrix_file)\n",
    "        saved_matrix = torch.load(matrix_path, map_location=device)\n",
    "        loss_sensitivity.matrix.matrix = saved_matrix\n",
    "        content_loss_impacts, style_loss_impacts = loss_sensitivity.run_dimension_dropping_experiment(contentV, styleV)\n",
    "        all_content_loss_impacts.append(content_loss_impacts)\n",
    "        all_style_loss_impacts.append(style_loss_impacts)\n",
    "\n",
    "    avg_content_loss_impacts = np.mean(all_content_loss_impacts, axis=0)\n",
    "    avg_style_loss_impacts = np.mean(all_style_loss_impacts, axis=0)\n",
    "\n",
    "    return avg_content_loss_impacts, avg_style_loss_impacts\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    vgg, dec, matrix = load_models(device)\n",
    "    \n",
    "    style_layers = ['r41']\n",
    "    content_layers = ['r41']\n",
    "    style_weight = 1e5\n",
    "    content_weight = 1.0\n",
    "\n",
    "    opt = Options()\n",
    "    loss_sensitivity = LossSensitivity(vgg, dec, matrix, style_layers, content_layers, \n",
    "                                       style_weight, content_weight, device)\n",
    "\n",
    "    style_dirs = [d for d in os.listdir(opt.matrixPath) if os.path.isdir(os.path.join(opt.matrixPath, d))]\n",
    "    all_content_loss_impacts = []\n",
    "    all_style_loss_impacts = []\n",
    "\n",
    "    for style_dir in style_dirs:\n",
    "        avg_content_loss_impacts, avg_style_loss_impacts = process_style_dir_dimension_dropping(style_dir, opt, loss_sensitivity, device)\n",
    "        all_content_loss_impacts.append(avg_content_loss_impacts)\n",
    "        all_style_loss_impacts.append(avg_style_loss_impacts)\n",
    "        plot_dimension_dropping_results(avg_content_loss_impacts, avg_style_loss_impacts, style_dir)\n",
    "\n",
    "    plot_comparison_dimension_dropping(style_dirs, all_content_loss_impacts, all_style_loss_impacts)\n",
    "    plot_overall_average_dimension_dropping(style_dirs, all_content_loss_impacts, all_style_loss_impacts)\n",
    "    plot_clustered_styles_dimension_dropping(style_dirs, all_content_loss_impacts, all_style_loss_impacts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from libs.Loader import Dataset\n",
    "from libs.models import encoder4, decoder4\n",
    "from libs.Criterion import LossCriterion\n",
    "from libs.Matrix import MulLayer\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LossSensitivity:\n",
    "    def __init__(self, vgg: nn.Module, dec: nn.Module, matrix: MulLayer,\n",
    "                 style_layers: List[str], content_layers: List[str],\n",
    "                 style_weight: float, content_weight: float, device: torch.device):\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.dec = dec.to(device)\n",
    "        self.matrix = matrix.to(device)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.criterion = LossCriterion(style_layers, content_layers, style_weight, content_weight)\n",
    "        self.device = device\n",
    "\n",
    "    def add_noise(self, matrix: torch.Tensor, sigma: float) -> torch.Tensor:\n",
    "        \"\"\"Adds random Gaussian noise to a matrix.\"\"\"\n",
    "        return matrix + torch.randn_like(matrix) * sigma\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, contentV: torch.Tensor, styleV: torch.Tensor) -> Tuple[dict, dict]:\n",
    "        return self.vgg(styleV), self.vgg(contentV)\n",
    "\n",
    "    def compute_loss(self, contentV: torch.Tensor, styleV: torch.Tensor, noisy_matrix: torch.Tensor) -> float:\n",
    "        sF, cF = self.forward(contentV, styleV)\n",
    "\n",
    "        transformed_features, _ = self.matrix(cF[self.style_layers[0]], sF[self.style_layers[0]])\n",
    "        b, c, h, w = transformed_features.size()\n",
    "        compressed_features = self.matrix.compress(transformed_features)\n",
    "\n",
    "        if noisy_matrix.size(1) != compressed_features.view(b, self.matrix.matrixSize, -1).size(1):\n",
    "            print(f\"Dimension mismatch: {noisy_matrix.size()} vs {compressed_features.size()}\")\n",
    "            return float('inf')\n",
    "\n",
    "        noisy_transfeature = torch.bmm(noisy_matrix, compressed_features.view(b, self.matrix.matrixSize, -1))\n",
    "        noisy_transfeature = noisy_transfeature.view(b, self.matrix.matrixSize, h, w)\n",
    "        noisy_transfeature = self.matrix.unzip(noisy_transfeature)\n",
    "\n",
    "        noisy_transfer = self.dec(noisy_transfeature).clamp(0, 1)\n",
    "        tF = self.vgg(noisy_transfer)\n",
    "\n",
    "        total_loss, _, _ = self.criterion(tF, sF, cF)\n",
    "        return total_loss.item()\n",
    "\n",
    "    def run_experiment(self, contentV: torch.Tensor, styleV: torch.Tensor,\n",
    "                       sigmas: np.ndarray, matrix: torch.Tensor) -> List[float]:\n",
    "        loss_values = []\n",
    "\n",
    "        for sigma in sigmas:\n",
    "            noisy_matrix = self.add_noise(matrix, sigma)\n",
    "\n",
    "            loss = self.compute_loss(contentV, styleV, noisy_matrix)\n",
    "            if loss == float('inf'):\n",
    "                print(f\"Skipping sigma {sigma} due to dimension mismatch.\")\n",
    "                continue\n",
    "\n",
    "            loss_values.append(loss)  # Store only the loss related to random noise\n",
    "\n",
    "        return loss_values\n",
    "\n",
    "\n",
    "def process_style_dir(style_dir: str, opt, loss_sensitivity: LossSensitivity,\n",
    "                      sigmas: np.ndarray, device: torch.device) -> List[float]:\n",
    "    style_path = os.path.join(opt.matrixPath, style_dir)\n",
    "    matrix_files = [f for f in os.listdir(style_path) if f.endswith('.pth')]\n",
    "\n",
    "    total_loss_values = np.zeros(len(sigmas))\n",
    "    num_matrices = 0\n",
    "\n",
    "    content_dataset = Dataset(opt.contentPath, opt.loadSize, opt.fineSize)\n",
    "    style_dataset = Dataset(opt.stylePath, opt.loadSize, opt.fineSize)\n",
    "\n",
    "    # Loop over all matrices saved for this style\n",
    "    for matrix_file in tqdm(matrix_files, desc=f\"Processing {style_dir}\"):\n",
    "        matrix_path = os.path.join(style_path, matrix_file)\n",
    "        saved_matrix = torch.load(matrix_path, map_location=device)\n",
    "\n",
    "        num_content_images = 0\n",
    "        loss_values_accumulated = np.zeros(len(sigmas))\n",
    "\n",
    "        # Loop over all content images\n",
    "        for contentV, _ in content_dataset:\n",
    "            contentV = contentV.unsqueeze(0).to(device)\n",
    "            styleV = style_dataset[0][0].unsqueeze(0).to(device)\n",
    "\n",
    "            # Run experiment for this content image\n",
    "            loss_values = loss_sensitivity.run_experiment(contentV, styleV, sigmas, saved_matrix)\n",
    "\n",
    "            # Accumulate losses for each sigma\n",
    "            loss_values_accumulated += np.array(loss_values)\n",
    "            num_content_images += 1\n",
    "\n",
    "        # Average loss values for each content image and add to total losses\n",
    "        total_loss_values += loss_values_accumulated / num_content_images\n",
    "        num_matrices += 1\n",
    "\n",
    "    # Average the accumulated results over the number of style matrices\n",
    "    avg_loss_values = total_loss_values / num_matrices\n",
    "\n",
    "    return avg_loss_values\n",
    "\n",
    "\n",
    "def plot_style_results(style_dir: str, sigmas: np.ndarray, avg_loss_values: np.ndarray):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(sigmas, avg_loss_values, '-o')\n",
    "    plt.xlabel('Sigma (Noise Level)')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title(f'Loss Sensitivity for Style: {style_dir}')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'noise_sensitivity_{style_dir}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_average_loss_trend(sigmas: np.ndarray, all_styles_loss_values: List[np.ndarray]):\n",
    "    \"\"\"\n",
    "    Plot the average loss trend across all styles with standard deviation.\n",
    "    \n",
    "    :param sigmas: Array of noise levels\n",
    "    :param all_styles_loss_values: List of loss values for each style\n",
    "    \"\"\"\n",
    "    avg_loss_values = np.mean(all_styles_loss_values, axis=0)\n",
    "    std_loss_values = np.std(all_styles_loss_values, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sigmas, avg_loss_values, '-', color='blue', label='Average Loss')\n",
    "    plt.fill_between(sigmas,\n",
    "                     avg_loss_values - std_loss_values,\n",
    "                     avg_loss_values + std_loss_values,\n",
    "                     alpha=0.3, color='lightblue', label='±1 Standard Deviation')\n",
    "\n",
    "    plt.xlabel('Sigma (Noise Level)')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title('Average Loss Trend Across All Styles')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('average_loss_trend.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Maximum average loss: {np.max(avg_loss_values):.4f}\")\n",
    "    print(f\"Maximum standard deviation: {np.max(std_loss_values):.4f}\")\n",
    "    print(f\"Sigma at maximum average loss: {sigmas[np.argmax(avg_loss_values)]:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load models\n",
    "    vgg, dec, matrix = load_models(device)\n",
    "\n",
    "    # Define options and parameters\n",
    "    opt = Options()\n",
    "    sigmas = np.linspace(0, 100, 10)  # Noise levels to test\n",
    "\n",
    "    # Initialize the loss sensitivity object with valid layer identifiers\n",
    "    loss_sensitivity = LossSensitivity(vgg, dec, matrix, style_layers=['r41'], content_layers=['r31'],\n",
    "                                       style_weight=1.0, content_weight=1.0, device=device)\n",
    "\n",
    "    # Loop over style directories\n",
    "    style_dirs = os.listdir(opt.matrixPath)\n",
    "    all_styles_loss_values = []\n",
    "\n",
    "    for style_dir in style_dirs:\n",
    "        # Process each style directory\n",
    "        avg_loss_values = process_style_dir(style_dir, opt, loss_sensitivity, sigmas, device)\n",
    "\n",
    "        # Plot results for the current style\n",
    "        plot_style_results(style_dir, sigmas, avg_loss_values)\n",
    "\n",
    "        # Store the results across all styles\n",
    "        all_styles_loss_values.append(avg_loss_values)\n",
    "\n",
    "    # Plot average loss trend across all styles\n",
    "    plot_average_loss_trend(sigmas, all_styles_loss_values)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
